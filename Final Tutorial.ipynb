{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Learning Objectives\n",
    "\n",
    "* Tools\n",
    "    * Pandas\n",
    "        * For Data storage\n",
    "        * Also for \"scraping\"\n",
    "    * Requests/BeautifulSoup4/LXML\n",
    "        * For scraping (DUH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Land Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Ethics Of Web-Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Am I allowed to take this data? Can I use it in my research? What can I use it for?\n",
    "\n",
    "* Site TOS\n",
    "* robots.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Can the site handle my requests?\n",
    "\n",
    "* Speed considerations\n",
    "* Small site?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Actual Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Background: Data Types & Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### list\n",
    "\n",
    "* Store multiple items (elements) in a single variable. \n",
    "* Elements are separated by commas.\n",
    "* \n",
    "\n",
    "Note that indices start at 0 in Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# initialize a list\n",
    "ex_list1 = [0, 1, 2]\n",
    "\n",
    "# print the list\n",
    "print(\"The 1st element of ex_list1 is\", ex_list1[0])\n",
    "\n",
    "# print length of a list\n",
    "print(\"List Length:\", len(ex_list1))\n",
    "\n",
    "# lists can contain \n",
    "ex_list2 = [\"Three\", [4,5]]\n",
    "\n",
    "# add two lists together \n",
    "ex_added_list = ex_list1 + ex_list2\n",
    "print(ex_added_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import lxml\n",
    "import lxml.html\n",
    "import cssselect\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pandas read html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### USDA FIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import data from USDA. Output is a LIST of tables\n",
    "usda_fips_page = pd.read_html(\"https://www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/?cid=nrcs143_013697\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print output to see what we have...\n",
    "usda_fips_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Strange output!\n",
    "usda_fips_page[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# looks like the first few dataframes are identical... this is strange but we only need one so it isn't a problem!\n",
    "usda_fips_page[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make variable for fips table we want\n",
    "usda_fips = usda_fips_page[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# let's examine the dataframe more closely to make sure everything is correct\n",
    "\n",
    "# change display options to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "usda_fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# it looks like the last row is the only incorrect one, so let's delete it\n",
    "usda_fips.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's confirm there are no other rows we need to drop. Check for nan values in any col pt. 1\n",
    "usda_fips.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check for nan values in any col pt. 2\n",
    "usda_fips.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# drop the last row\n",
    "\n",
    "# this is the output we want\n",
    "usda_fips.drop(3232)\n",
    "\n",
    "# now replace the existing \"usda_fips\" dataframe with the version missing the last row (inplace=True)\n",
    "usda_fips.drop(3232, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Wikipedia FIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import data from wikipedia. Again, output is a LIST of tables\n",
    "fips_page = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's print the output and take a look...\n",
    "fips_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# first element?\n",
    "fips_page[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fips_page[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# we want the 2nd table (recall: indices start at 0)\n",
    "fips = fips_page[1]\n",
    "\n",
    "# remove all hyperlinks (these look like \"... County [h]\", etc.)\n",
    "fips['County or equivalent'] = fips['County or equivalent'].str.replace(r\"\\[.*\\]\",\"\")\n",
    "\n",
    "# convert to uppercase\n",
    "fips['County or equivalent'] = fips['County or equivalent'].apply(lambda x: x.upper())\n",
    "fips['State or equivalent'] = fips['State or equivalent'].apply(lambda x: x.upper())\n",
    "\n",
    "# replace \"St.\" with \"Saint\"\n",
    "fips['County or equivalent'] = [x.replace('ST.','SAINT') for x in fips['County or equivalent']]\n",
    "\n",
    "# remove everything after a comma in a county name (e.g. \"ANCHORAGE, MUNICIPALITY OF\")\n",
    "fips['County or equivalent'] = [x.split(',')[0] for x in fips['County or equivalent']]\n",
    "\n",
    "# replace DC info to correspond to GeoCov19 format\n",
    "dc_loc = fips[fips['County or equivalent']=='DISTRICT OF COLUMBIA'].index.tolist()[0]\n",
    "fips['State or equivalent'].loc[dc_loc] = 'WASHINGTON, D.C.'\n",
    "fips['County or equivalent'].loc[dc_loc] = 'WASHINGTON'\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Save FIPS dataframe to file\n",
    "\n",
    "FIPS_SAVE_PATH = \n",
    "\n",
    "fips.to_csv(FIPS_SAVE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Request  + BeautifulSoup\n",
    "\n",
    "# Red-bellied Snake (Wikipedia) Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Red-bellied Snake (Wikipedia) Text Analysis\n",
    "\n",
    "Hypothetical: we want to get a list of all words, and their frequency, from this wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wiki_snake_url = \"https://en.wikipedia.org/wiki/Red-bellied_black_snake\"\n",
    "wiki_snake_response = requests.get(wiki_snake_url)\n",
    "\n",
    "# printing \"200\" means that the page was successfully downloaded!\n",
    "print(wiki_snake_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#let's examine the output... which turns out to be a mess (this is where BeautifulSoup comes in handy)\n",
    "wiki_snake_response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wiki_snake_soup = BeautifulSoup(wiki_snake_response.text, 'html.parser')\n",
    "\n",
    "# BeautifulSoup gives us a more readable format (barely)\n",
    "wiki_snake_soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# let's make this even more readable, so we can identify parts of the text we want to gather\n",
    "\n",
    "# this shows us all html with an \"p\" tag (paragraphs), but we just want the text\n",
    "for paragraph in wiki_snake_soup.find_all('p'):\n",
    "    print(paragraph.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[paragraph.text for paragraph in wiki_snake_soup.find_all('p')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# we're almost there, we just need to combine this output\n",
    "\n",
    "wiki_snake_text = ' '.join([paragraph.text for paragraph in wiki_snake_soup.find_all('p')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wiki_snake_text\n",
    "# looks good, but we should remove '\\n'\n",
    "\n",
    "wiki_snake_text = wiki_snake_text.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_snake_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# now let's count occurences of each word\n",
    "# import nltk\n",
    "\n",
    "nltk.FreqDist(wiki_snake_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# looks like we need to split the text first:\n",
    "split_wiki_snake_text = wiki_snake_text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wiki_snake_word_freqs = nltk.FreqDist(split_wiki_snake_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#what are we working with?\n",
    "type(wiki_snake_word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wiki_snake_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_snake_words = []\n",
    "wiki_snake_freqs = []\n",
    "\n",
    "for word in wiki_snake_word_freqs:\n",
    "    wiki_snake_words.append(word)\n",
    "    wiki_snake_freqs.append(wiki_snake_word_freqs[word])\n",
    "\n",
    "print(wiki_snake_words)\n",
    "print(wiki_snake_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# finally, we convert it to a pandas dataframe so we can easily save it in .csv format\n",
    "wiki_snake_output = pd.DataFrame({'Word': wiki_snake_words, 'Frequency': wiki_snake_freqs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wiki_snake_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wiki_snake_output.to_csv(r\"Red Bellied Snake Words & Frequencies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request  + BeautifulSoup\n",
    "\n",
    "# lichess.org user data\n",
    "\n",
    "Let's get the information of every top \"bullet\" chess player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the top bullet players\n",
    "players_df = pd.read_html(\"https://lichess.org/player/top/200/bullet\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del players_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df.rename(columns={1:'User', \n",
    "                       2:'Rating',\n",
    "                       3:'Abs. Rating Change'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to split the \"User\" column, otherwise we get urls like \"https://lichess.org/@/GM RebeccaHarris\" instead of \"https://lichess.org/@/RebeccaHarris\"\n",
    "\n",
    "players_df['Title'] = players_df['User'].str.split('\\xa0').str[0]\n",
    "\n",
    "players_df['User'] = players_df['User'].str.split('\\xa0').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Abs. Rating Change</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RebeccaHarris</td>\n",
       "      <td>3121</td>\n",
       "      <td>17.0</td>\n",
       "      <td>GM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nihalsarin2004</td>\n",
       "      <td>3060</td>\n",
       "      <td>2.0</td>\n",
       "      <td>GM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heisenberg01</td>\n",
       "      <td>3021</td>\n",
       "      <td>5.0</td>\n",
       "      <td>FM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chessbrahs</td>\n",
       "      <td>3010</td>\n",
       "      <td>3.0</td>\n",
       "      <td>GM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexander_Zubov</td>\n",
       "      <td>3010</td>\n",
       "      <td>23.0</td>\n",
       "      <td>GM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Avenger82</td>\n",
       "      <td>2775</td>\n",
       "      <td>21.0</td>\n",
       "      <td>FM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2775</td>\n",
       "      <td>9.0</td>\n",
       "      <td>badbadger95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2774</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Estoiko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2773</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chesszxj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>MFmauricioRamirez</td>\n",
       "      <td>2773</td>\n",
       "      <td>3.0</td>\n",
       "      <td>FM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  User  Rating  Abs. Rating Change        Title\n",
       "0        RebeccaHarris    3121                17.0           GM\n",
       "1       nihalsarin2004    3060                 2.0           GM\n",
       "2         Heisenberg01    3021                 5.0           FM\n",
       "3           chessbrahs    3010                 3.0           GM\n",
       "4      Alexander_Zubov    3010                23.0           GM\n",
       "..                 ...     ...                 ...          ...\n",
       "195          Avenger82    2775                21.0           FM\n",
       "196                NaN    2775                 9.0  badbadger95\n",
       "197                NaN    2774                18.0      Estoiko\n",
       "198                NaN    2773                 NaN     chesszxj\n",
       "199  MFmauricioRamirez    2773                 3.0           FM\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "players_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RebeccaHarris\n",
      "https://lichess.org/@/RebeccaHarris\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def get_user_info(lichess_user_id):\n",
    "    print(user_ID)\n",
    "    \n",
    "    player_url = \"https://lichess.org/@/\" + user_ID\n",
    "    print(player_url)\n",
    "    \n",
    "    player_response = requests.get(player_url)\n",
    "    \n",
    "    player_soup = BeautifulSoup(player_response.text, 'html.parser')\n",
    "    \n",
    "    player_info = player_soup.find_all(\".thin:nth-child(1)\")\n",
    "    \n",
    "    print(player_info)\n",
    "\n",
    "\n",
    "for user_ID in [players_df['User'].iloc[0]]:\n",
    "  \n",
    "    get_user_info(user_ID)\n",
    "\n",
    "    time.sleep(.5) \n",
    "    \n",
    "#     print(user_ID)\n",
    "    \n",
    "#     player_url = \"https://lichess.org/@/\" + user_ID\n",
    "#     print(player_url)\n",
    "    \n",
    "#     player_response = requests.get(player_url)\n",
    "    \n",
    "#     player_soup = BeautifulSoup(player_response.text, 'html.parser')\n",
    "    \n",
    "#     player_info = player_soup.find_all(\".thin:nth-child(1)\")\n",
    "    \n",
    "#     print(player_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p class=\"thin\">Member since 1 Sep 2016</p>]\n",
      "['Member since 1 Sep 2016']\n",
      " 1 Sep 2016\n"
     ]
    }
   ],
   "source": [
    "# let's get their join date\n",
    "member_since = player_soup.select('.thin:nth-child(1)')\n",
    "\n",
    "# let's see its format\n",
    "print(member_since)\n",
    "\n",
    "# we need to get the text from here and clean it\n",
    "# member_since.text\n",
    "\n",
    "# we forgot that member_since is a list! (even though it just has one element, it's' still a list).\n",
    "# let's get the text from its only element:\n",
    "print([x.text for x in member_since])\n",
    "\n",
    "# great! we just need to clean this to get useful information out - we're almost there\n",
    "def get_membership_date(string):\n",
    "    split_str = string.split('since')[1]\n",
    "    return split_str\n",
    "\n",
    "#test our function:\n",
    "print(get_membership_date([x.text for x in member_since][0]))\n",
    "\n",
    "# ok the function looks good, let's put it into our "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAYBE TRY XPATH HERE?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
